# Chunked Ingestion Orchestrator

This tool splits large CSV datasets into smaller chunks and orchestrates their sequential import into a Postgres container. It is designed for robustness, handling partial failures and enabling execution resumption.

## Features
-   **Low Memory Usage**: Streams large input files row-by-row.
-   **Resumable**: Skips chunks that have already been successfully imported (checks for `.done` files).
-   **Safe**: Validates input, prevents double-processing, and provides dry-runs.
-   **Audit**: Logs commands for every chunk (Bash & PowerShell formats).

## Usage

**Prerequisites**:
-   Node.js (v14+)
-   Docker container running (e.g., `postgres_db`)
-   Input CSV file

### 1. Dry Run (Split & Inspect)
Use this to generate chunks and see what commands *would* be run.

```bash
cd backend
node scripts/chunked_copy_orchestrator.js \
  --input ./data/dataset_for_copy.csv \
  --out-dir ./data/chunks \
  --chunk-size 200000 \
  --container postgres_db
```

**Output**:
-   Creates `./data/chunks/chunk_0001.csv`, `chunk_0002.csv`, etc.
-   Prints `docker cp` and `docker exec` commands to the console.

### 2. Execute Import
Use the `--execute true` flag to actually run the import commands.

```bash
node scripts/chunked_copy_orchestrator.js \
  --execute true \
  --chunk-size 200000 \
  --yes
```

-   **--yes**: Skips the confirmation prompt.
-   **Resume**: If the script is interrupted, simply run the same command again. It will detect existing `.done` marker files in `data/chunks/` and skip them.

### 3. Resetting / Re-running
To force a re-run of all chunks:
-   Delete the chunks directory: `rm -rf data/chunks`
-   OR delete just the marker files: `rm data/chunks/*.done`

## Troubleshooting

### "Container Not Found"
-   Ensure your Postgres container is running: `docker ps`
-   Check the name matches the `--container` argument (default: `postgres_db`, sometimes `root-postgres-1` or similar depending on Compose).

### Import Fails on Specific Chunk
The script will stop immediately and print the failing command.
1.  Check the specific error message (e.g., data type mismatch, constraint violation).
2.  Fix the data in the specific chunk file (e.g., `data/chunks/chunk_0005.csv`).
3.  Run the printed manual command to retry just that chunk.
4.  Once successful, create a marker file manually (`touch data/chunks/chunk_0005.csv.done`) or re-run the orchestrator (it will see your manual fix if you didn't mark it done? No, you should let the orchestrator re-run it).

## Manual Command Reference

If you need to copy/import a specific file manually, use the commands generated by the script logs.

**Example (Bash):**
```bash
docker cp "data/chunks/chunk_0001.csv" postgres_db:/tmp/chunk_0001.csv
docker exec -i postgres_db psql -U postgres -d postgres -c "\copy sales(...) FROM '/tmp/chunk_0001.csv' WITH (FORMAT csv, HEADER true, NULL '')"
```

**Example (PowerShell):**
```powershell
docker cp "data/chunks/chunk_0001.csv" postgres_db:/tmp/chunk_0001.csv
docker exec -i postgres_db psql -U postgres -d postgres -c "\copy sales(...) FROM '/tmp/chunk_0001.csv' WITH (FORMAT csv, HEADER true, NULL '')"
```
